{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2ae4cfaa-e895-4cfb-8750-5922fd9f0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "#keras.layers.Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97afe127-bec6-41b2-b52d-f32639c50d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def pclip(p):\n",
    "    \"\"\"Clip recall probability to avoid numerical issues.\"\"\"\n",
    "    return p.clip(0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def hclip(h):\n",
    "    min_half_life = 15.0 / (24 * 60)  # 15 minutes in days\n",
    "    max_half_life = 274.0   \n",
    "    \"\"\"Clip half-life to a reasonable range.\"\"\"\n",
    "    return h.clip(min_half_life, max_half_life)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b86d645-8db6-4907-a100-5e4254c38dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "dff = pd.merge(pd.merge(df_words, df, on = 'lexeme_id', how='inner'), df_users, on = ['user_id', 'lang_combination'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d858cdc-e930-44a2-ac43-92362b9aa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['lexeme_id' ,'gender', 'def', 'tense', 'POS', 'person', 'number', 'word', 'session_seen', 'session_correct', 'avg_user_p_recall', 'timestamp', 'user_id', 'learning_language', 'ui_language']\n",
    "dff.drop(columns=cols_to_drop, inplace=True)\n",
    "dff.dropna(inplace=True)\n",
    "\n",
    "\n",
    "dff['delta'] = dff['delta']/(60*60*24) # convert time delta to days\n",
    "dff['avg_delta'] = dff['avg_delta']/(60*60*24) \n",
    "dff['std_delta'] = dff['std_delta']/(60*60*24)\n",
    "dff['p_recall'] = pclip(dff['p_recall'])\n",
    "dff['half_life'] = hclip(-dff['delta']/np.log2(dff['p_recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e06b2c84-b914-493f-b89e-f6b6b5ac9266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_len</th>\n",
       "      <th>tags_list</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>p_recall</th>\n",
       "      <th>delta</th>\n",
       "      <th>history_seen</th>\n",
       "      <th>history_correct</th>\n",
       "      <th>h_recall</th>\n",
       "      <th>lang_combination</th>\n",
       "      <th>avg_delta</th>\n",
       "      <th>std_delta</th>\n",
       "      <th>avg_h_recall</th>\n",
       "      <th>half_life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>['vblex', 'pri', 'p3', 'sg']</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.069016</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>en-de</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>0.034457</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>274.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>['vblex', 'pri', 'p3', 'sg']</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>en-de</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>0.034457</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>['vblex', 'pri', 'p3', 'sg']</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>en-de</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>0.034457</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>5.214388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>['vblex', 'pri', 'p3', 'sg']</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>en-de</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>0.034457</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>0.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['vblex', 'pri', 'p3', 'sg']</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>en-de</td>\n",
       "      <td>1.009879</td>\n",
       "      <td>1.633872</td>\n",
       "      <td>0.914070</td>\n",
       "      <td>14.359623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_len                     tags_list  SUBTLEX  p_recall     delta  \\\n",
       "0         5  ['vblex', 'pri', 'p3', 'sg']   3391.0    0.9999  0.069016   \n",
       "1         5  ['vblex', 'pri', 'p3', 'sg']   3391.0    0.0001  0.002928   \n",
       "2         5  ['vblex', 'pri', 'p3', 'sg']   3391.0    0.9999  0.000752   \n",
       "3         5  ['vblex', 'pri', 'p3', 'sg']   3391.0    0.5000  0.000313   \n",
       "4         5  ['vblex', 'pri', 'p3', 'sg']   3391.0    0.9999  0.002072   \n",
       "\n",
       "   history_seen  history_correct  h_recall lang_combination  avg_delta  \\\n",
       "0             8                6  0.750000            en-de   0.035931   \n",
       "1            14               12  0.857143            en-de   0.035931   \n",
       "2            15               12  0.800000            en-de   0.035931   \n",
       "3            16               13  0.812500            en-de   0.035931   \n",
       "4            15               15  1.000000            en-de   1.009879   \n",
       "\n",
       "   std_delta  avg_h_recall   half_life  \n",
       "0   0.034457      0.890225  274.000000  \n",
       "1   0.034457      0.890225    0.010417  \n",
       "2   0.034457      0.890225    5.214388  \n",
       "3   0.034457      0.890225    0.010417  \n",
       "4   1.633872      0.914070   14.359623  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c933ebf-5ef6-443b-af86-0d4eb4387a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding tags and langs \n",
    "# tag_encoder = LabelEncoder()\n",
    "# lang_encoder = LabelEncoder()\n",
    "\n",
    "# dff['tags_list'] = tag_encoder.fit_transform(dff['tags_list'])\n",
    "# dff['lang_combination'] = lang_encoder.fit_transform(dff['lang_combination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "322c5992-95b8-4d23-9394-3302a4ade04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    categorical_features = df.select_dtypes(include='O').columns\n",
    "    numeric_features = df.select_dtypes(exclude=['O']).columns.drop(['p_recall', 'half_life', 'delta'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "    return df, categorical_features, numeric_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c7885884-1b32-4ae6-bf1a-4155e6f19e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_1 = dff.copy()\n",
    "dff_1, categorical_features, numeric_features = prepare_dataset(dff_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e778e4bb-6557-4c24-9cba-a01bb815fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "def split(df, numeric_features): \n",
    "    \n",
    "    X = df.drop(columns=['p_recall', 'half_life'])\n",
    "    y = df[['p_recall', 'half_life']]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = sklearn_train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    X_train_delta = X_train['delta']\n",
    "    X_test_delta = X_test['delta'] \n",
    "    \n",
    "    X_train_numerical = X_train[numeric_features]\n",
    "    X_test_numerical = X_test[numeric_features]\n",
    "\n",
    "    # In case we use encodings \n",
    "    X_train_tags = X_train['tags_list']\n",
    "    X_train_langs = X_train['lang_combination']\n",
    "    X_test_tags = X_test['tags_list']\n",
    "    X_test_langs = X_test['lang_combination']\n",
    "    \n",
    "\n",
    "    # In case we use half-life regression\n",
    "    y_train_p_recall = y_train['p_recall']\n",
    "    y_train_half_life = y_train['half_life']\n",
    "    y_test_p_recall = y_test['p_recall']\n",
    "    y_test_half_life = y_test['half_life']\n",
    "\n",
    "\n",
    "    # Embeddings \n",
    "    # return df, X_train_tags, X_train_langs, X_train_numerical, X_test_tags, X_test_langs, X_test_numerical, X_test, y_train, y_test, y_train_half_life, y_train_p_recall, y_test_half_life, y_test_p_recall\n",
    "\n",
    "    # No embeddings \n",
    "    return df, X_train_delta, X_test_delta, X_train_numerical, X_test_numerical, y_train_p_recall, y_train_half_life, y_test_p_recall, y_test_half_life\n",
    "\n",
    "# Embeddings \n",
    "# df, X_train_tags, X_train_langs, X_train_numerical, X_test_tags, X_test_langs, X_test_numerical, X_test,  y_train, y_test, y_train_half_life, y_train_p_recall, y_test_half_life, y_test_p_recall = split(dff_1.sample(frac=0.1), numeric_features)\n",
    "\n",
    "\n",
    "# No embeddings\n",
    "df, X_train_delta, X_test_delta, X_train_numerical, X_test_numerical, y_train_p_recall, y_train_half_life, y_test_p_recall, y_test_half_life = split(dff_1.sample(frac=0.1), numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b7d6ddc4-7782-4a1d-b08b-e17e4d162ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('X_train_tags_size', X_train_tags.shape)\n",
    "# print('X_train_langs_size', X_train_langs.shape)\n",
    "# print('X_train_numerical_size', X_train_numerical.shape)\n",
    "# print('X_test_tags_size', X_test_tags.shape)\n",
    "# print('X_test_langs_size', X_test_langs.shape)\n",
    "# print('X_test_numerical_size', X_test_numerical.shape)\n",
    "# print('y_train_half_life_size', y_train_half_life.shape)\n",
    "# print('y_train_p_recall_size', y_train_p_recall.shape)\n",
    "# print('y_test_half_life_size', y_test_half_life.size)\n",
    "# print('y_test_p_recall_size', y_test_p_recall.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e566998-4b71-4fc5-bb52-9a893bc3d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "# len_tags = len(np.unique(df_final['tags_list']))\n",
    "# len_langs = len(np.unique(df_final['lang_combination']))\n",
    "\n",
    "# embedding_tags_size = int(min(np.ceil((len_tags)/2), 50))\n",
    "# embedding_langs_size = int(min(np.ceil((len_langs)/2), 50))\n",
    "\n",
    "# tags_input = Input(shape=(1,))  # Reshape input to (None, 1)\n",
    "# langs_input = Input(shape=(1,))  # Reshape input to (None, 1)\n",
    "\n",
    "# tags_embedded = Embedding(input_dim=len_tags, output_dim=embedding_tags_size)(tags_input)  \n",
    "# langs_embedded = Embedding(input_dim=len_langs, output_dim=embedding_langs_size)(langs_input)  \n",
    "\n",
    "# flattened_tags = Flatten()(tags_embedded)\n",
    "# flattened_langs = Flatten()(langs_embedded)\n",
    "\n",
    "numerical_input = Input(shape=(len(numeric_features),))  # Should be 11\n",
    "delta_input = Input(shape=(1,))  # Time delta (Δ)\n",
    "\n",
    "\n",
    "# Concatenate layers\n",
    "# conc = Concatenate()([flattened_tags, flattened_langs, numerical_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "77cae0bc-b67e-4f69-8e11-92cf6cd5301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architectture\n",
    "# input_dim = X_train.shape[1] \n",
    "hidden_dim = 4         \n",
    "l2wt = 0.1              # L2 regularization weight\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "x = Dense(hidden_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(l2wt))(numerical_input)\n",
    "\n",
    "# For half-life\n",
    "half_life_output = Dense(1, activation=\"relu\", name=\"half_life\")(x) \n",
    "p_recall_output = Lambda(lambda inputs: tf.pow(2.0, -inputs[0] / (inputs[1] + 1e-6)), \n",
    "                         name=\"p_recall\")([delta_input, half_life_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "43187307-02b3-4ae2-9d3d-c7273e9a76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhlr_loss(y_true, y_pred):\n",
    "    h_true, p_true = y_true[0], y_true[1]\n",
    "    h_pred, p_pred = y_pred[0], y_pred[1]\n",
    "\n",
    "    slh = tf.reduce_mean(tf.square(h_true - h_pred)) # half-life loss \n",
    "    slp = tf.reduce_mean(tf.square(p_true - p_pred)) # p_recall loss \n",
    "\n",
    "    return slp + slh \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "293b1641-b989-43c9-b236-07bc97d9c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_numerical shape: (1000440, 8)\n",
      "X_train_delta shape: (1000440,)\n"
     ]
    }
   ],
   "source": [
    "# print(\"Max tag index:\", X_train_tags.max(), \"Embedding input_dim:\", len_tags)\n",
    "# print(\"Max lang index:\", X_train_langs.max(), \"Embedding input_dim:\", len_langs)\n",
    "\n",
    "# print(\"Tags Input Shape:\", tags_input.shape)\n",
    "# print(\"Langs Input Shape:\", langs_input.shape)\n",
    "# print(\"Numerical Input Shape:\", numerical_input.shape)\n",
    "\n",
    "# print(\"Flattened tags Embedded Shape:\", flattened_tags.shape)\n",
    "# print(\"Flattened Langs Embedded Shape:\", flattened_langs.shape)\n",
    "# print(\"Flattened Numerical Input Shape:\", numerical_input.shape)\n",
    "\n",
    "# print(\"X_train_tags shape:\", X_train_tags.shape)    # Should be (batch_size, 1)\n",
    "# print(\"X_train_langs shape:\", X_train_langs.shape)  # Should be (batch_size, 1)\n",
    "print(\"X_train_numerical shape:\", X_train_numerical.shape)  \n",
    "print(\"X_train_delta shape:\", X_train_delta.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "51a0bccc-e1c5-454d-8978-8ea1d6e27604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31264/31264 - 13s - 415us/step - half_life_MAE: 127.9265 - half_life_loss: 41797.1992 - loss: 41810.4844 - p_recall_MAE: 0.2450 - p_recall_loss: 0.4176\n",
      "Epoch 2/10\n",
      "31264/31264 - 12s - 392us/step - half_life_MAE: 119.4258 - half_life_loss: 30019.0469 - loss: 30040.8086 - p_recall_MAE: 0.1160 - p_recall_loss: 0.1612\n",
      "Epoch 3/10\n",
      "31264/31264 - 12s - 392us/step - half_life_MAE: 118.8874 - half_life_loss: 29926.8965 - loss: 29955.5391 - p_recall_MAE: 0.1159 - p_recall_loss: 0.1617\n",
      "Epoch 4/10\n",
      "31264/31264 - 16s - 505us/step - half_life_MAE: 118.4693 - half_life_loss: 29776.2129 - loss: 29811.2930 - p_recall_MAE: 0.1158 - p_recall_loss: 0.1551\n",
      "Epoch 5/10\n",
      "31264/31264 - 12s - 391us/step - half_life_MAE: 118.2274 - half_life_loss: 29796.4863 - loss: 29837.2266 - p_recall_MAE: 0.1157 - p_recall_loss: 0.1590\n",
      "Epoch 6/10\n",
      "31264/31264 - 17s - 530us/step - half_life_MAE: 118.0567 - half_life_loss: 29762.4414 - loss: 29807.9453 - p_recall_MAE: 0.1157 - p_recall_loss: 0.1597\n",
      "Epoch 7/10\n",
      "31264/31264 - 12s - 397us/step - half_life_MAE: 117.8548 - half_life_loss: 29621.0254 - loss: 29670.7441 - p_recall_MAE: 0.1156 - p_recall_loss: 0.1594\n",
      "Epoch 8/10\n",
      "31264/31264 - 12s - 392us/step - half_life_MAE: 117.7927 - half_life_loss: 29689.2598 - loss: 29743.0820 - p_recall_MAE: 0.1156 - p_recall_loss: 0.1602\n",
      "Epoch 9/10\n",
      "31264/31264 - 17s - 531us/step - half_life_MAE: 117.7103 - half_life_loss: 29708.2422 - loss: 29764.9375 - p_recall_MAE: 0.1156 - p_recall_loss: 0.1629\n",
      "Epoch 10/10\n",
      "31264/31264 - 12s - 395us/step - half_life_MAE: 117.6184 - half_life_loss: 29556.6074 - loss: 29615.4668 - p_recall_MAE: 0.1155 - p_recall_loss: 0.1591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x35c29c890>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[numerical_input, delta_input], outputs=[half_life_output, p_recall_output])\n",
    "\n",
    "# model = Model(inputs=[tags_input, langs_input, numerical_input], outputs=output)\n",
    "\n",
    "model.compile(loss=nhlr_loss, optimizer= Adam(learning_rate=learning_rate), metrics=['MAE', 'MAE'])\n",
    "\n",
    "model.fit([X_train_numerical, X_train_delta], [y_train_half_life, y_train_p_recall], epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "# Without half lfie \n",
    "# model.fit([X_train_tags, X_train_langs, X_train_numerical], y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3781b244-4446-4aa1-a79f-d62d48df995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7816/7816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 247us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_half_life, y_pred_p_recall  = model.predict([X_test_numerical, X_test_delta])\n",
    "\n",
    "y_test['p_recall_pred'] = y_pred_p_recall\n",
    "y_test['half_life_pred'] = y_pred_half_life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2a6adb3c-315b-4359-8262-84d5b80c7309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MAE - p_recall: 0.1175\n"
     ]
    }
   ],
   "source": [
    "mae_p = np.mean(np.abs(y_test['p_recall'] - y_test['p_recall_pred']))\n",
    "mae_h = np.mean(np.abs(y_test['half_life'] - y_test['half_life_pred']))\n",
    "\n",
    "print(f\"Final MAE - p_recall: {mae_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1b055626-5f5e-441d-b333-f0be9b6f70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tag index: 400 Embedding input_dim: 361\n",
      "Max lang index: 7 Embedding input_dim: 8\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398f546-bf99-4641-84c7-e7be5ada8bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

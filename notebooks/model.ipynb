{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = '13 million Duolingo student learning traces.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/', filename))\n",
    "\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 1 \n",
    "\"\"\" Instead of the sparse indicator variables used here, it may be better to decompose lexeme tags \n",
    "into denser and more generic features of tag components (e.g., part of speech, tense, gender, case), \n",
    "and also use corpus frequency, word length, etc.\"\"\"\n",
    "lexeme_filepath = os.path.normpath(os.path.join(current_dir, '../data/', 'lexeme_reference.csv'))\n",
    "lexeme_reference = pd.read_csv(lexeme_filepath, sep = ';', header=None, on_bad_lines='warn', \n",
    "                               names=[\"tag\", \"type\", \"description\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animacy', 'adjective', 'POS', 'other', 'propernoun', 'tense',\n",
       "       'def', 'gender', 'case', 'number', 'person'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme_reference['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['V','allele']] = df['V'].str.split('-',expand=True)\n",
    "df['word'] = df['lexeme_string'].str.split(\"/\").str[0]\n",
    "df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           lernt/lernen<vblex><pri><p3><sg>\n",
       "1              die/die<det><def><f><sg><nom>\n",
       "2                   mann/mann<n><m><sg><nom>\n",
       "3                   frau/frau<n><f><sg><nom>\n",
       "4             das/das<det><def><nt><sg><nom>\n",
       "                          ...               \n",
       "12854140               the/the<det><def><sp>\n",
       "12854141                eat/eat<vblex><pres>\n",
       "12854142                  bread/bread<n><sg>\n",
       "12854143            drink/drink<vblex><pres>\n",
       "12854144                  water/water<n><sg>\n",
       "Name: lexeme_string, Length: 12854145, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lexeme_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

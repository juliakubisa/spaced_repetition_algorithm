{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c735b02c-eeab-417e-892c-1d02851886cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a559aaf-d957-45e2-8b9b-c91f41ea2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6ce2d-dd0c-4b45-bd26-b61c36da34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.merge(df_words, on = 'lexeme_id', how='inner')\n",
    "df_2 = df_1.merge(df_users, on = ['user_id', 'lang_combination'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248324be-133a-430c-bd45-8b1759324374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a995d6-c590-4e91-a72c-b08cc7f4082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE DATA FOR ML \n",
    "dff = df_2.drop(columns=['timestamp', 'lexeme_id', 'word', 'user_id', 'session_seen', 'session_correct', 'avg_user_p_recall'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93f230-ce9b-4a81-b9bd-b626683733a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ac800-5694-40f9-a09f-efa1bf6edc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two approaches to handle tags \n",
    "# 1) Store tags as list, but delete rows where there are less than x occurences \n",
    "dff_1 = dff.copy()\n",
    "tag_counts = dff_1['tags_list'].value_counts()\n",
    "rare_threshold = 1000\n",
    "dff_1['tags_list'] = dff_1['tags_list'].apply(lambda x: x if tag_counts[x] > rare_threshold else 'rare')\n",
    "\n",
    "dff_1.drop(columns=['POS', 'person', 'number', 'gender', 'tense', 'def'], inplace=True)\n",
    "dff_1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661516bf-efb9-4709-8273-aeaa4a32ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between variables\n",
    "sns.heatmap(dff_1.select_dtypes(exclude='O').corr(), annot=False, cmap='coolwarm')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e2b44-4317-4ad9-b5bb-8266f0426fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Store each tag as column, replace NaN values with a placeholder for categorical columns\n",
    "categorical_cols = dff.select_dtypes(include=['object']).columns.tolist()\n",
    "dff_2 = dff.copy()\n",
    "dff_2[categorical_cols] = dff_2[categorical_cols].fillna('missing')\n",
    "dff_2.drop(columns=['tags_list'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf972f7-8e20-434e-832d-83cc297ac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF -> iteratevily delete columns that had VIF > 10\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "    \n",
    "dff_VIF = dff_1.drop(columns=['p_recall', 'avg_h_recall', 'history_correct']).sample(2000000)\n",
    "vif = calculate_vif(dff_VIF.select_dtypes(exclude='O'))\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f3a2f-57ab-4426-8bb4-af7204853577",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_features = [\"avg_h_recall\", \"avg_user_p_recall\", 'history_correct']\n",
    "correlation_with_target = dff_1[removed_features + [\"p_recall\"]].corr()[\"p_recall\"]\n",
    "print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba469d7-e4e3-46a3-afe8-b79ff26b76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0035124-cf14-4b2e-90b7-4e63f501e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After deleting VIF\n",
    "dff_final = dff_1.sample(1000000)\n",
    "# dff_final = dff_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a62146-3a0b-4a0b-97e0-0d04113656dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming skewed variables\n",
    "dff_final['delta'] = np.sqrt(dff_final['delta']/(60*60*24))\n",
    "dff_final['avg_delta'] = np.sqrt(dff_final['avg_delta']/(60*60*24))\n",
    "dff_final['history_seen'] = np.sqrt(dff_final['history_seen'])\n",
    "dff_final['history_correct'] = np.sqrt(dff_final['history_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081b9b2-7200-4145-a885-c39971c1a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of target variable \n",
    "\n",
    "# Logit transformation \n",
    "def logit_transform(y, epsilon=1e-10):\n",
    "    y = np.clip(y, epsilon, 1 - epsilon)\n",
    "    return np.log(y / (1 - y))\n",
    "\n",
    "def inverse_logit_transform(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "\n",
    "# Log transformation \n",
    "def log_transform(y, epsilon=1e-10):\n",
    "    return np.log(y + epsilon)\n",
    "\n",
    "def inverse_log_transform(y):\n",
    "    return np.exp(y)\n",
    "\n",
    "\n",
    "# Box-Cox transformation\n",
    "power_transformer = PowerTransformer(method='box-cox', standardize=False)\n",
    "def power_transform(y, epsilon=1e-10):\n",
    "    return power_transformer.fit_transform(y+epsilon) \n",
    "\n",
    "def inverse_power_transform(y, epsilon=1e-10):\n",
    "    return power_transformer.inverse_transform(y+epsilon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30999554-179f-4380-bd14-808e19634955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df without word features \n",
    "dff_t = dff_final.drop(columns=['tags_list', 'word_len', 'SUBTLEX'], errors='ignore')\n",
    "\n",
    "# DF original \n",
    "dff_s = dff_final.drop(columns=['SUBTLEX', 'tags_list', 'word_len', 'avg_delta', 'std_delta', 'avg_h_recall', 'lang_combination', 'h_recall'], errors='ignore')\n",
    "\n",
    "# DF without user columns \n",
    "dff_u = dff_final.drop(columns=['avg_delta', 'std_delta', 'avg_h_recall', 'h_recall'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faebfda-81e3-4416-8387-954f4885899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(df):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical variables\n",
    "    \"\"\" \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    ohe_data = ohe.fit_transform(df[categorical_cols])\n",
    "    ohe_df = pd.DataFrame(ohe_data, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "    df_encoded = pd.concat([df.select_dtypes(exclude='O'), ohe_df], axis=1)\n",
    "    df_encoded.dropna(inplace=True)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7538c-7f93-403e-8f9a-4790989e8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    X = df.drop(columns='p_recall')\n",
    "    y = df['p_recall']\n",
    "    X_train, X_test, y_train, y_test = sklearn_train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2c4d4-1f08-4dab-a413-d2aa8e9d603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_y(prediction): \n",
    "    return np.clip(prediction, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b114ed-a5df-40f2-8c02-8acce707ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_encoded = ohe(dff_final)\n",
    "dff_t_encoded = ohe(dff_t)\n",
    "dff_s_encoded = ohe(dff_s)\n",
    "dff_u_encoded = ohe(dff_u)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataset(dff_encoded)\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = split_dataset(dff_t_encoded)\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = split_dataset(dff_s_encoded)\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = split_dataset(dff_u_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97e5ba-151d-4e3a-a7fd-b719e6c67e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model_type=\"linear\", alpha=1.0, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a pipeline for regression with optional feature selection.\n",
    "    \n",
    "    Returns:\n",
    "    - Pipeline object\n",
    "    \"\"\"\n",
    "    if model_type == \"ridge\":\n",
    "        model = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "\n",
    "    steps = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        # ('polynomial_features', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "        ('model', model)\n",
    "        # ('model', TransformedTargetRegressor(regressor=model, \n",
    "        #                                     func=log_transform,\n",
    "        #                                     inverse_func=inverse_log_transform))\n",
    "    ]\n",
    "    return Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6b701-7bb4-4efb-afac-228e18a6ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train and evaluate a pipeline, returning metrics.\n",
    "    \"\"\"\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on both train and test\n",
    "    y_train_pred = cap_y(pipeline.predict(X_train))\n",
    "    y_test_pred = cap_y(pipeline.predict(X_test)) \n",
    "\n",
    "    # Calculate metrics \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    \n",
    "    print(f\"{name}: Train R2 = {round(train_r2, 4)}, Test R2 = {round(test_r2,4)}, Train MAE = {round(train_mae,4)}, Test MAE = {round(test_mae, 4)}\")\n",
    "    return y_train_pred, y_test_pred, test_mae, test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f81ffd-92da-4595-9ba4-22293ec0ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configurations for Pipelines\n",
    "pipelines_config = [\n",
    "    {\"name\": \"Linear Regression\", \"model_type\": \"linear\"},\n",
    "    {\"name\": \"Ridge Regression\", \"model_type\": \"ridge\", \"alpha\": 100},\n",
    "    {\"name\": \"Lasso Regression\", \"model_type\": \"lasso\"},\n",
    "]\n",
    "\n",
    "def regression_results(pipelines_config, X_train, X_test, y_train, y_test, importance='No'):\n",
    "    results = []\n",
    "    importances = [] \n",
    "    \n",
    "    for config in pipelines_config:\n",
    "        pipeline = create_pipeline(**config)\n",
    "        y_test_pred, y_train_pred, mae, r2 = evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test, name=config[\"name\"])\n",
    "        results.append((config[\"name\"], mae, r2))\n",
    "\n",
    "        if isinstance(importance, str) and importance == 'Yes':\n",
    "            model = pipeline.named_steps[\"model\"]\n",
    "            importance = model.coef_\n",
    "            importances.append((config[\"name\"], X_train.columns, importance))\n",
    "\n",
    "    importance_df = pd.DataFrame(importances, columns = ['Model', 'Feature', 'Importance'])\n",
    "    results_df = pd.DataFrame(results, columns=['Model', 'Test MAE', 'Test R2'])   \n",
    "    return results_df, importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c939dd-6686-4646-812b-51b164d4cc7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results for all features')\n",
    "results_all = regression_results(pipelines_config, X_train, X_test, y_train, y_test, importance='No')\n",
    "\n",
    "print('Results after deleting word features')\n",
    "results_without_word = regression_results(pipelines_config, X_train_t, X_test_t, y_train_t, y_test_t, importance='Yes')\n",
    "\n",
    "print('Results without word and user features')\n",
    "results_original = regression_results(pipelines_config, X_train_s, X_test_s, y_train_s, y_test_s, importance='Yes')\n",
    "\n",
    "print('Results without user features') \n",
    "results_without_user = regression_results(pipelines_config, X_train_u, X_test_u, y_train_u, y_test_u, importance='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4390217-b44b-481b-8e64-3683dfce7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MSE Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_all['Model'], results_all['Test MSE'], color='skyblue')\n",
    "plt.title('Test MSE Comparison')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "# # Test R2 Plot\n",
    "# plt.subplot(1, 2,1)\n",
    "# plt.bar(results_all['Model'], results_all['Test R2'], color='lightgreen')\n",
    "# plt.title('Test R2 Comparison')\n",
    "# plt.ylabel('R2 Score')\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174dcc4-149b-46b8-b4d6-ba756ccafd78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if residuals are normally distributed \n",
    "sns.distplot((y_test-y_pred_capped),bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d158dfe-86d3-467a-817f-a4d266813c1b",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

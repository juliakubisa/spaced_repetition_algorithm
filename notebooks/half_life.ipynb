{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ee633-14e5-4fca-81b2-bda39cce72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half-Life as implemented by Duolingo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40860931-8de0-4f58-99e0-98d42682107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed1b7c-e173-48bc-9bf9-552d7b3ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "\n",
    "df_1 = df.merge(df_words, on = 'lexeme_id', how='inner')\n",
    "df_2 = df_1.merge(df_users, on = ['user_id', 'lang_combination'], how='inner')\n",
    "dff = df_2.drop(columns=['timestamp', 'lexeme_id', 'word', 'user_id', 'POS', 'person', 'number', 'gender', 'tense', 'def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4309558-59fd-4d78-b51b-e7654bfba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'lang':[1,2,3], 'halo':[0.01, 0, 0.99]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a2111-0907-43e0-8d29-85929abc9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to dataset before fitting \n",
    "dff['p_recall'] = pclip(dff['p_recall'])\n",
    "dff['delta'] = dff['delta']/(60*60*24) # convert time delta to days\n",
    "dff['half_life'] = hclip(-dff['delta']/math.log(dff['p_recall'],2)))\n",
    "\n",
    "dff_final = dff.drop(columns=['learning_language', 'ui_language'])\n",
    "\n",
    "\n",
    "# original                 \n",
    "# h = hclip(-t/(math.log(p, 2)))\n",
    "# lang = '%s->%s' % (row['ui_language'], row['learning_language'])\n",
    "# lexeme_id = row['lexeme_id']\n",
    "# lexeme_string = row['lexeme_string']\n",
    "# timestamp = int(row['timestamp'])\n",
    "# user_id = row['user_id']\n",
    "# seen = int(row['history_seen'])\n",
    "# right = int(row['history_correct'])\n",
    "# wrong = seen - right\n",
    "# right_this = int(row['session_correct'])\n",
    "# wrong_this = int(row['session_seen']) - right_this\n",
    "\n",
    "# they left only lang combination \n",
    "# used lexeme id \n",
    "# used timestamp and lexeme_string and user_id \n",
    "# h = hclip(-t/(math.log(p, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c23870-3c10-4802-8d52-46d996be04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data instance object\n",
    "instances = list()\n",
    "Instance = namedtuple('Instance', 'p_recal t feature_vector h_recall a lang right wrong ts user_id lexeme'.split())\n",
    "\n",
    "# feature vector is a list of (feature, value) tuples\n",
    "fv = []\n",
    "fv.append((intern('right'), math.sqrt(1+right)))\n",
    "fv.append((intern('wrong'), math.sqrt(1+wrong)))\n",
    "fv.append((intern('%s:%s' % (row['learning_language'], lexeme_string)), 1.))\n",
    "\n",
    "instances.append(Instance(p, t, fv, h, (right+2.)/(seen+4.), lang, right_this, wrong_this, timestamp, user_id, lexeme_string))\n",
    "    # read data set\n",
    "    trainset, testset = read_data(args.input_file, args.method, args.b, args.l, args.max_lines)\n",
    "    sys.stderr.write('|train| = %d\\n' % len(trainset))\n",
    "    sys.stderr.write('|test|  = %d\\n' % len(testset))\n",
    "\n",
    "    # train model & print preliminary evaluation info\n",
    "    model = SpacedRepetitionModel(method=args.method, omit_h_term=args.t)\n",
    "    model.train(trainset)\n",
    "    model.eval(testset, 'test')\n",
    "\n",
    "# to namedtuples \n",
    "list(df.itertuples(name='Row', index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c41c7b25-134a-49c2-8b8b-d775b192ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "min_half_life = 15.0 / (24 * 60)  # 15 minutes in days\n",
    "max_half_life = 274.0            # 9 months\n",
    "LN2 = math.log(2)\n",
    "\n",
    "# Utility functions\n",
    "def pclip(p):\n",
    "    \"\"\"Clip recall probability to avoid numerical issues.\"\"\"\n",
    "    return np.clip(p, 0.001, 0.9999)\n",
    "\n",
    "def hclip(h):\n",
    "    \"\"\"Clip half-life to a reasonable range.\"\"\"\n",
    "    return np.clip(h, min_half_life, max_half_life)\n",
    "\n",
    "def mae(l1, l2):\n",
    "    # mean average error\n",
    "    return mean([abs(l1[i] - l2[i]) for i in range(len(l1))])\n",
    "\n",
    "def mean(lst):\n",
    "    # the average of a list\n",
    "    return float(sum(lst))/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893839da-8b8f-4e19-8b8c-4bbbdeb4499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HLR Model\n",
    "class HalfLifeRegression:\n",
    "    def __init__(self, learning_rate=0.001, hlwt=0.01, l2wt=0.1, sigma=1.):\n",
    "        self.weights = defaultdict(float)  # Feature weights\n",
    "        self.fcounts = defaultdict(int)    # Feature counts for adaptive learning rates\n",
    "        self.learning_rate = learning_rate # Base learning rate\n",
    "        self.hlwt = hlwt                   # Weight for half-life loss\n",
    "        self.l2wt = l2wt                   # L2 regularization weight\n",
    "        self.sigma = sigma\n",
    "        if initial_weights is not None:\n",
    "            self.weights.update(initial_weights)\n",
    "\n",
    "    def halflife(self, inst):\n",
    "        \"\"\"Compute predicted half-life based on feature vector.\"\"\"\n",
    "        try:\n",
    "            dp = sum([self.weights[k]*x_k for (k, x_k) in inst.fv]) # where inst.fb is the feature vector\n",
    "            return hclip(2 ** dp)\n",
    "        except: \n",
    "            return MAX_HALF_LIFE\n",
    "            \n",
    "\n",
    "    def predict(self, inst):\n",
    "        \"\"\"Predict recall probability and half-life.\"\"\"\n",
    "        h = self.halflife(inst)\n",
    "        p = 2 ** (-inst.t / h) # where inst.t is the delta t \n",
    "        return pclip(p), h\n",
    "\n",
    "                \n",
    "     def train_update(self, inst):\n",
    "        \"\"\"Update weights using one training instance.\"\"\"\n",
    "        p_pred, h_pred = self.predict(inst)\n",
    "\n",
    "        # Compute gradients\n",
    "        dlp_dw = 2*(p_pred - inst.p)*(LN2 ** 2)*p_pred*(inst.t/h_pred)\n",
    "        dlh_dw = 2*(h_pred - inst.h)*LN2*h_pred\n",
    "\n",
    "        # Update weights\n",
    "        for (k, x_k) in inst.fv:\n",
    "            rate = (1./(1+inst.p)) * self.lrate / math.sqrt(1 + self.fcounts[k])\n",
    "            self.weights[k] -= rate * dlp_dw * x_k  # Update for recall probability loss\n",
    "            self.weights[k] -= rate * self.hlwt * dlh_dw * x_k  # Update for half-life loss\n",
    "            self.weights[k] -= rate * self.l2wt * self.weights[k] / self.sigma**2  # L2 regularization\n",
    "            self.fcounts[k] += 1\n",
    "\n",
    "\n",
    "    def train(self, trainset):\n",
    "        random.shuffle(trainset)\n",
    "        for instance in trainset:\n",
    "            self.train_update(inst)\n",
    "\n",
    "    def evaluate(self, testset):\n",
    "        \"\"\"Evaluate the model on a test dataset.\"\"\"\n",
    "        results = {'p': [], 'h': [], 'pp': [], 'hh': [], 'slp': [], 'slh': []}\n",
    "        for instance in testset:\n",
    "            slp, slh, p, h = self.losses(inst)\n",
    "            results['p'].append(inst.p)     # ground truth\n",
    "            results['h'].append(inst.h)\n",
    "            results['pp'].append(p)         # predictions\n",
    "            results['hh'].append(h)\n",
    "            results['slp'].append(slp)      # loss function values\n",
    "            results['slh'].append(slh)\n",
    "            mae_p = mae(results['p'], results['pp'])\n",
    "            mae_h = mae(results['h'], results['hh'])\n",
    "            total_slp = sum(results['slp'])\n",
    "            total_slh = sum(results['slh'])\n",
    "            total_l2 = sum([x**2 for x in self.weights.values()])\n",
    "            total_loss = total_slp + self.hlwt*total_slh + self.l2wt*total_l2\n",
    "        print(f\"SLP Loss: {total_slp}, SLH Loss: {total_slh}, MAE_P: {mae_p}, MAE_H: {mae_h}, total loss {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42723394-33b0-4a81-9615-d8a8941a7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HalfLifeRegression()\n",
    "model.train(trainset)\n",
    "model.evaluate(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

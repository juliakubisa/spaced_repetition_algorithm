{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4ee633-14e5-4fca-81b2-bda39cce72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half-Life as implemented by Duolingo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40860931-8de0-4f58-99e0-98d42682107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.metrics import r2_score\n",
    "from sys import intern\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17034490-502c-4d6d-ab4f-237d8dd653fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['timestamp', 'lexeme_id', 'word', 'user_id', 'POS', 'person', \n",
    "                  'number', 'gender', 'tense', 'def', 'session_seen', 'session_correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ed1b7c-e173-48bc-9bf9-552d7b3ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "\n",
    "dff = pd.merge(pd.merge(df_words, df, on = 'lexeme_id', how='inner'), df_users, on = ['user_id', 'lang_combination'], how='inner')\n",
    "dff.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4309558-59fd-4d78-b51b-e7654bfba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "min_half_life = 15.0 / (24 * 60)  # 15 minutes in days\n",
    "max_half_life = 274.0            # 9 months\n",
    "LN2 = math.log(2)\n",
    "\n",
    "# Utility functions\n",
    "def pclip(p):\n",
    "    \"\"\"Clip recall probability to avoid numerical issues.\"\"\"\n",
    "    return p.clip(0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def hclip(h):\n",
    "    \"\"\"Clip half-life to a reasonable range.\"\"\"\n",
    "    return h.clip(min_half_life, max_half_life)\n",
    "\n",
    "def mae(l1, l2):\n",
    "    # mean average error\n",
    "    return mean([abs(l1[i] - l2[i]) for i in range(len(l1))])\n",
    "\n",
    "def mean(lst):\n",
    "    # the average of a list\n",
    "    return float(sum(lst))/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c63137d-f146-4c0e-8f14-87bc753d919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p_recall', 'delta', 'learning_language', 'ui_language', 'history_seen',\n",
       "       'history_correct', 'h_recall', 'lang_combination', 'word_len',\n",
       "       'tags_list', 'SUBTLEX', 'avg_user_p_recall', 'avg_delta', 'std_delta',\n",
       "       'avg_h_recall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e0a2111-0907-43e0-8d29-85929abc9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to dataset before fitting \n",
    "dff.dropna(inplace=True)\n",
    "dff['p_recall'] = pclip(dff['p_recall'])\n",
    "dff['half_life'] = hclip(-dff['delta']/np.log2(dff['p_recall']))\n",
    "\n",
    "dff['delta'] = dff['delta']/(60*60*24) # convert time delta to days\n",
    "dff['avg_delta'] = dff['avg_delta']/(60*60*24) \n",
    "dff['std_delta'] = dff['std_delta']/(60*60*24)\n",
    "dff['history_wrong'] = dff['history_seen'] - dff['history_correct']\n",
    "\n",
    "tag_counts = dff['tags_list'].value_counts()\n",
    "rare_threshold = 1000\n",
    "dff['tags_list'] = dff['tags_list'].apply(lambda x: x if tag_counts[x] > rare_threshold else 'rare')\n",
    "\n",
    "\n",
    "dff_final = dff.drop(columns=['learning_language_y', 'ui_language_y', 'learning_language_x', 'ui_language_x', 'avg_user_p_recall'], errors='ignore')\n",
    "dff_final.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4115b8d2-8cbd-44a0-ba22-99f2b2239bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instance = namedtuple('Instance', 'p_recall delta fv half_life'.split())\n",
    "\n",
    "def create_instances_from_dataframe(df):\n",
    "    df = df.sample(frac=1)\n",
    "    instances = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Build the feature vector for this row\n",
    "        fv = []\n",
    "        # fv.append((intern('history_seen'), np.sqrt(1 + row['history_seen'])))\n",
    "        fv.append((intern('history_correct'), np.sqrt(1 + row['history_correct'])))\n",
    "        fv.append((intern('history_wrong'), np.sqrt(1 + row['history_wrong'])))\n",
    "        # fv.append((intern('h_recall'), np.sqrt(1+row['h_recall'])))\n",
    "        fv.append((intern('word_len'), row['word_len']))\n",
    "        fv.append((intern('lang_comb:' + row['lang_combination']), 1.0))\n",
    "        fv.append((intern('avg_delta'), row['avg_delta']))\n",
    "        # fv.append((intern('SUBTLEX'), row['SUBTLEX']))\n",
    "        fv.append((intern('std_delta'), row['std_delta']))\n",
    "        fv.append((intern('avg_h_recall'), row['avg_h_recall']))\n",
    "        fv.append((intern('tags_list:' + row['tags_list']), 1.0))\n",
    "\n",
    "        instance = Instance(\n",
    "            p_recall=row['p_recall'],\n",
    "            delta=row['delta'],\n",
    "            fv=fv,\n",
    "            half_life=row['half_life']\n",
    "        )\n",
    "\n",
    "        instances.append(instance)\n",
    "        \n",
    "    splitpoint = int(0.8 * len(instances))\n",
    "    return instances[:splitpoint], instances[splitpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce17c4-2824-4d59-8a98-175c922ed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "\n",
    "class HalfLifeRegression:\n",
    "    def __init__(self, learning_rate=0.001, hlwt=0.01, l2wt=0.1, sigma=1., initial_weights=None):\n",
    "        self.weights = defaultdict(float)  # Feature weights\n",
    "        self.fcounts = defaultdict(int)    # Feature counts for adaptive learning rates\n",
    "        self.learning_rate = learning_rate # Base learning rate\n",
    "        self.hlwt = hlwt                   # Weight for half-life loss\n",
    "        self.l2wt = l2wt                   # L2 regularization weight\n",
    "        self.sigma = sigma                 # Sigma value for L2 regularization\n",
    "        if initial_weights is not None:\n",
    "            self.weights.update(initial_weights)\n",
    "\n",
    "    def halflife(self, inst):\n",
    "        \"\"\"Compute predicted half-life based on feature vector.\"\"\"\n",
    "        try:\n",
    "            dp = sum([self.weights[k] * x_k for (k, x_k) in inst.fv])  # where inst.fv is the feature vector\n",
    "            dp = np.clip(dp, -50, 50)\n",
    "            with np.errstate(over='raise'):\n",
    "                return hclip(np.exp2(dp))\n",
    "        except:\n",
    "            return max_half_life  # Return a default max value if an error occurs\n",
    "\n",
    "    def predict(self, inst):\n",
    "        \"\"\"Predict recall probability and half-life.\"\"\"\n",
    "        h_pred = self.halflife(inst)\n",
    "        p_pred = 2 ** (-inst.delta / h_pred)\n",
    "        return pclip(p_pred), h_pred  \n",
    "\n",
    "    \n",
    "    def train_update(self, inst):\n",
    "        \"\"\"Update weights using one training instance.\"\"\"\n",
    "        p_pred, h_pred = self.predict(inst)\n",
    "\n",
    "        # Compute gradients\n",
    "        dlp_dw = 2 * (p_pred - inst.p_recall) * (LN2 ** 2) * p_pred * (inst.delta / h_pred)\n",
    "        dlh_dw = 2 * (h_pred - inst.half_life) * LN2 * h_pred\n",
    "\n",
    "        # Update weights\n",
    "        for (k, x_k) in inst.fv:\n",
    "            rate = (1. / (1 + inst.p_recall)) * self.learning_rate / math.sqrt(1 + self.fcounts[k])\n",
    "            # Update for recall probability loss\n",
    "            self.weights[k] -= rate * dlp_dw * x_k  \n",
    "            \n",
    "            # Update forh half-life loss \n",
    "            self.weights[k] -= rate * self.hlwt * dlh_dw * x_k  \n",
    "\n",
    "            # L2 regularization\n",
    "            self.weights[k] -= rate * self.l2wt * self.weights[k] / self.sigma**2  \n",
    "            self.fcounts[k] += 1\n",
    "\n",
    "\n",
    "    def train(self, trainset):\n",
    "        random.shuffle(trainset)  # Shuffle the training set\n",
    "        for inst in trainset:\n",
    "            self.train_update(inst)\n",
    "\n",
    "    def losses(self, inst):\n",
    "        p_pred, h_pred = self.predict(inst)\n",
    "        slp = (inst.p_recall - p_pred)**2\n",
    "        slh = (inst.half_life - h_pred)**2\n",
    "        return slp, slh, p_pred, h_pred\n",
    "\n",
    "    def evaluate(self, testset):\n",
    "            \"\"\"Evaluate the model on a test dataset.\"\"\"\n",
    "            results = {'p': [], 'h': [], 'pp': [], 'hh': [], 'slp': [], 'slh': []}\n",
    "            for inst in testset:\n",
    "                slp, slh, p_pred, h_pred = self.losses(inst)\n",
    "                results['p'].append(inst.p_recall)\n",
    "                results['h'].append(inst.half_life)\n",
    "                results['pp'].append(p_pred)\n",
    "                results['hh'].append(h_pred)\n",
    "                results['slp'].append(slp)\n",
    "                results['slh'].append(slh)\n",
    "                results['fv'].append(inst.fv) \n",
    "                results['delta'].append(inst.delta) \n",
    "    \n",
    "    \n",
    "            mae_p = mae(results['p'], results['pp'])\n",
    "            mae_h = mae(results['h'], results['hh'])\n",
    "            total_slp = sum(results['slp'])\n",
    "            total_slh = sum(results['slh'])\n",
    "            total_l2 = sum([x ** 2 for x in self.weights.values()])\n",
    "            total_loss = total_slp + self.hlwt * total_slh + self.l2wt * total_l2\n",
    "            r2val = r2_score(results['p'], results['pp'])\n",
    "            print(f\" MAE_P: {mae_p}, MAE_H: {mae_h}, R2: {r2val}\")\n",
    "            return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42723394-33b0-4a81-9615-d8a8941a7407",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m create_instances_from_dataframe(dff)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m HalfLifeRegression()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(testset)\n",
      "Cell \u001b[0;32mIn[60], line 60\u001b[0m, in \u001b[0;36mHalfLifeRegression.train\u001b[0;34m(self, trainset)\u001b[0m\n\u001b[1;32m     58\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(trainset)  \u001b[38;5;66;03m# Shuffle the training set\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inst \u001b[38;5;129;01min\u001b[39;00m trainset:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 37\u001b[0m, in \u001b[0;36mHalfLifeRegression.train_update\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update weights using one training instance.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     p_pred, h_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     dlp_dw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (p_pred \u001b[38;5;241m-\u001b[39m inst\u001b[38;5;241m.\u001b[39mp_recall) \u001b[38;5;241m*\u001b[39m (LN2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m p_pred \u001b[38;5;241m*\u001b[39m (inst\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m/\u001b[39m h_pred)\n",
      "Cell \u001b[0;32mIn[60], line 32\u001b[0m, in \u001b[0;36mHalfLifeRegression.predict\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m     29\u001b[0m p_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39minst\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m/\u001b[39m h_pred)  \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Clip probabilities within bounds\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_pred\u001b[49m\u001b[43m)\u001b[49m, h_pred\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mpclip\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpclip\u001b[39m(p):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clip recall probability to avoid numerical issues.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m(\u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.9999\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'clip'"
     ]
    }
   ],
   "source": [
    "trainset, testset = create_instances_from_dataframe(dff)\n",
    "model = HalfLifeRegression()\n",
    "model.train(trainset)\n",
    "results = model.evaluate(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

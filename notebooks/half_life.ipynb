{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ee633-14e5-4fca-81b2-bda39cce72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half-Life as implemented by Duolingo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40860931-8de0-4f58-99e0-98d42682107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "from sys import intern\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ed1b7c-e173-48bc-9bf9-552d7b3ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "\n",
    "df_1 = df.merge(df_words, on = 'lexeme_id', how='inner')\n",
    "df_1['lang_combination'] = df_1['ui_language']+ '-' + df_1['learning_language']\n",
    "df_2 = df_1.merge(df_users, on = ['user_id', 'lang_combination'], how='inner')\n",
    "dff = df_2.drop(columns=['timestamp', 'lexeme_id', 'word', 'user_id', 'POS', 'person', 'number', 'gender', 'tense', 'def', 'session_seen', 'session_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d4309558-59fd-4d78-b51b-e7654bfba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "min_half_life = 15.0 / (24 * 60)  # 15 minutes in days\n",
    "max_half_life = 274.0            # 9 months\n",
    "LN2 = math.log(2)\n",
    "\n",
    "# Utility functions\n",
    "def pclip(p):\n",
    "    \"\"\"Clip recall probability to avoid numerical issues.\"\"\"\n",
    "    return min(max(p, 0.0001), 0.9999)\n",
    "\n",
    "def hclip(h):\n",
    "    \"\"\"Clip half-life to a reasonable range.\"\"\"\n",
    "    return min(max(h, min_half_life), max_half_life)\n",
    "\n",
    "def mae(l1, l2):\n",
    "    # mean average error\n",
    "    return mean([abs(l1[i] - l2[i]) for i in range(len(l1))])\n",
    "\n",
    "def mean(lst):\n",
    "    # the average of a list\n",
    "    return float(sum(lst))/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "210728eb-7a6c-4dcb-b97c-b6464ef45f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c63137d-f146-4c0e-8f14-87bc753d919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p_recall', 'delta', 'learning_language', 'ui_language', 'history_seen',\n",
       "       'history_correct', 'h_recall', 'lang_combination', 'word_len',\n",
       "       'tags_list', 'SUBTLEX', 'avg_user_p_recall', 'avg_delta', 'std_delta',\n",
       "       'avg_h_recall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0a2111-0907-43e0-8d29-85929abc9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to dataset before fitting \n",
    "dff['p_recall'] = pclip(dff['p_recall'])\n",
    "dff['delta'] = dff['delta']/(60*60*24) # convert time delta to days\n",
    "dff['avg_delta'] = dff['avg_delta']/(60*60*24) \n",
    "dff['std_delta'] = dff['std_delta']/(60*60*24) \n",
    "dff['half_life'] = hclip(-dff['delta']/np.log2(dff['p_recall']))\n",
    "\n",
    "tag_counts = dff['tags_list'].value_counts()\n",
    "rare_threshold = 1000\n",
    "dff['tags_list'] = dff['tags_list'].apply(lambda x: x if tag_counts[x] > rare_threshold else 'rare')\n",
    "\n",
    "\n",
    "dff_final = dff.drop(columns=['learning_language_y', 'ui_language_y', 'learning_language_x', 'ui_language_x', 'avg_user_p_recall'], errors='ignore')\n",
    "dff_final.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048b24b5-7d04-4e01-b8a1-09edada44b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_recall</th>\n",
       "      <th>delta</th>\n",
       "      <th>learning_language</th>\n",
       "      <th>ui_language</th>\n",
       "      <th>history_seen</th>\n",
       "      <th>history_correct</th>\n",
       "      <th>h_recall</th>\n",
       "      <th>lang_combination</th>\n",
       "      <th>word_len</th>\n",
       "      <th>tags_list</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>avg_delta</th>\n",
       "      <th>std_delta</th>\n",
       "      <th>avg_h_recall</th>\n",
       "      <th>half_life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>5.143600</td>\n",
       "      <td>es</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>en-es</td>\n",
       "      <td>4</td>\n",
       "      <td>['pr']</td>\n",
       "      <td>111241.0</td>\n",
       "      <td>2.475405e+06</td>\n",
       "      <td>2.879771e+06</td>\n",
       "      <td>0.954897</td>\n",
       "      <td>274.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.069016</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>en-de</td>\n",
       "      <td>5</td>\n",
       "      <td>['vblex', 'pri', 'p3', 'sg']</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>3.104417e+03</td>\n",
       "      <td>2.977079e+03</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>274.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.069016</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>en-de</td>\n",
       "      <td>3</td>\n",
       "      <td>['det', 'def', 'f', 'sg']</td>\n",
       "      <td>2484854.0</td>\n",
       "      <td>3.104417e+03</td>\n",
       "      <td>2.977079e+03</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>0.166289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.069016</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>en-de</td>\n",
       "      <td>4</td>\n",
       "      <td>['n', 'm', 'sg']</td>\n",
       "      <td>222707.0</td>\n",
       "      <td>3.104417e+03</td>\n",
       "      <td>2.977079e+03</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>0.406157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.069016</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>en-de</td>\n",
       "      <td>4</td>\n",
       "      <td>['n', 'f', 'sg']</td>\n",
       "      <td>143725.0</td>\n",
       "      <td>3.104417e+03</td>\n",
       "      <td>2.977079e+03</td>\n",
       "      <td>0.890225</td>\n",
       "      <td>0.214384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_recall     delta learning_language ui_language  history_seen  \\\n",
       "0  0.999900  5.143600                es          en             3   \n",
       "1  0.999900  0.069016                de          en             8   \n",
       "2  0.750000  0.069016                de          en             6   \n",
       "3  0.888889  0.069016                de          en             6   \n",
       "4  0.800000  0.069016                de          en             8   \n",
       "\n",
       "   history_correct  h_recall lang_combination  word_len  \\\n",
       "0                3  1.000000            en-es         4   \n",
       "1                6  0.750000            en-de         5   \n",
       "2                5  0.833333            en-de         3   \n",
       "3                5  0.833333            en-de         4   \n",
       "4                6  0.750000            en-de         4   \n",
       "\n",
       "                      tags_list    SUBTLEX     avg_delta     std_delta  \\\n",
       "0                        ['pr']   111241.0  2.475405e+06  2.879771e+06   \n",
       "1  ['vblex', 'pri', 'p3', 'sg']     3391.0  3.104417e+03  2.977079e+03   \n",
       "2     ['det', 'def', 'f', 'sg']  2484854.0  3.104417e+03  2.977079e+03   \n",
       "3              ['n', 'm', 'sg']   222707.0  3.104417e+03  2.977079e+03   \n",
       "4              ['n', 'f', 'sg']   143725.0  3.104417e+03  2.977079e+03   \n",
       "\n",
       "   avg_h_recall   half_life  \n",
       "0      0.954897  274.000000  \n",
       "1      0.890225  274.000000  \n",
       "2      0.890225    0.166289  \n",
       "3      0.890225    0.406157  \n",
       "4      0.890225    0.214384  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4115b8d2-8cbd-44a0-ba22-99f2b2239bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instance = namedtuple('Instance', 'p_recall delta fv half_life'.split())\n",
    "\n",
    "def create_instances_from_dataframe(df):\n",
    "    instances = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Build the feature vector for this row\n",
    "        fv = []\n",
    "        # fv.append((intern('history_seen'), np.sqrt(1 + row['history_seen'])))\n",
    "        # fv.append((intern('history_correct'), np.sqrt(1 + row['history_correct'])))\n",
    "        fv.append((intern('h_recall'), row['h_recall']))\n",
    "        # fv.append((intern('word_len'), row['word_len']))\n",
    "        # fv.append((intern('lang_comb:' + row['lang_combination']), 1.0))\n",
    "        # fv.append((intern('avg_delta'), row['avg_delta']))\n",
    "        # fv.append((intern('SUBTLEX'), row['SUBTLEX']))\n",
    "        # fv.append((intern('std_delta'), row['std_delta']))\n",
    "        # fv.append((intern('avg_h_recall'), row['avg_h_recall']))\n",
    "        # fv.append((intern('tags_list:' + row['tags_list']), 1.0))\n",
    "\n",
    "        instance = Instance(\n",
    "            p_recall=row['p_recall'],\n",
    "            delta=row['delta'],\n",
    "            fv=fv,\n",
    "            half_life=row['half_life']\n",
    "        )\n",
    "\n",
    "        instances.append(instance)\n",
    "        \n",
    "    splitpoint = int(0.8 * len(instances))\n",
    "    return instances[:splitpoint], instances[splitpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "534924f9-2c82-4126-a2c0-5fdeb556ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "\n",
    "class HalfLifeRegression:\n",
    "    def __init__(self, learning_rate=0.001, hlwt=0.01, l2wt=0.1, sigma=1., initial_weights=None):\n",
    "        self.weights = defaultdict(float)  # Feature weights\n",
    "        self.fcounts = defaultdict(int)    # Feature counts for adaptive learning rates\n",
    "        self.learning_rate = learning_rate # Base learning rate\n",
    "        self.hlwt = hlwt                   # Weight for half-life loss\n",
    "        self.l2wt = l2wt                   # L2 regularization weight\n",
    "        self.sigma = sigma                 # Sigma value for L2 regularization\n",
    "        if initial_weights is not None:\n",
    "            self.weights.update(initial_weights)\n",
    "\n",
    "    def halflife(self, inst):\n",
    "        \"\"\"Compute predicted half-life based on feature vector.\"\"\"\n",
    "        try:\n",
    "            dp = sum([self.weights[k] * x_k for (k, x_k) in inst.fv])  # where inst.fv is the feature vector\n",
    "            # dp = np.clip(dp, -50, 50)\n",
    "            return hclip(2 ** dp)  \n",
    "        except Exception as e:\n",
    "            return max_half_life  # Return a default max value if an error occurs\n",
    "\n",
    "    def predict(self, inst):\n",
    "        \"\"\"Predict recall probability and half-life.\"\"\"\n",
    "        h_pred = self.halflife(inst)\n",
    "        p_pred = 2 ** (-inst.delta / h_pred)  # Calculate recall probability\n",
    "        return pclip(p_pred), h_pred  # Clip probabilities within bounds\n",
    "\n",
    "    \n",
    "    def train_update(self, inst):\n",
    "        \"\"\"Update weights using one training instance.\"\"\"\n",
    "        p_pred, h_pred = self.predict(inst)\n",
    "\n",
    "        # Compute gradients\n",
    "        dlp_dw = 2 * (p_pred - inst.p_recall) * (LN2 ** 2) * p_pred * (inst.delta / h_pred)\n",
    "        dlh_dw = 2 * (h_pred - inst.half_life) * LN2 * h_pred\n",
    "\n",
    "        # Update weights\n",
    "        for (k, x_k) in inst.fv:\n",
    "            rate = (1. / (1 + inst.p_recall)) * self.learning_rate / math.sqrt(1 + self.fcounts[k])\n",
    "            self.weights[k] -= rate * dlp_dw * x_k  # Update for recall probability loss\n",
    "            self.weights[k] -= rate * self.hlwt * dlh_dw * x_k  # Update for half-life loss\n",
    "            self.weights[k] -= rate * self.l2wt * self.weights[k] / self.sigma**2  # L2 regularization\n",
    "            self.fcounts[k] += 1\n",
    "\n",
    "\n",
    "    def train(self, trainset):\n",
    "        random.shuffle(trainset)  # Shuffle the training set\n",
    "        for inst in trainset:\n",
    "            self.train_update(inst)\n",
    "\n",
    "    def losses(self, inst):\n",
    "        p_pred, h_pred = self.predict(inst)\n",
    "        slp = (inst.p_recall - p_pred)**2\n",
    "        slh = (inst.half_life - h_pred)**2\n",
    "        return slp, slh, p_pred, h_pred\n",
    "\n",
    "    def evaluate(self, testset):\n",
    "        \"\"\"Evaluate the model on a test dataset.\"\"\"\n",
    "        results = {'p': [], 'h': [], 'pp': [], 'hh': [], 'slp': [], 'slh': []}\n",
    "        for inst in testset:\n",
    "            slp, slh, p_pred, h_pred = self.losses(inst)\n",
    "\n",
    "            results['p'].append(inst.p_recall)\n",
    "            results['h'].append(inst.half_life)\n",
    "            results['pp'].append(p_pred)\n",
    "            results['hh'].append(h_pred)\n",
    "            results['slp'].append(slp)\n",
    "            results['slh'].append(slh)\n",
    "\n",
    "        mae_p = mae(results['p'], results['pp'])\n",
    "        mae_h = mae(results['h'], results['hh'])\n",
    "        total_slp = sum(results['slp'])\n",
    "        total_slh = sum(results['slh'])\n",
    "        total_l2 = sum([x ** 2 for x in self.weights.values()])\n",
    "        total_loss = total_slp + self.hlwt * total_slh + self.l2wt * total_l2\n",
    "\n",
    "        print(f\"SLP Loss: {total_slp}, SLH Loss: {total_slh}, MAE_P: {mae_p}, MAE_H: {mae_h}, Total Loss: {total_loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "42723394-33b0-4a81-9615-d8a8941a7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLP Loss: 701946.193645011, SLH Loss: 97000960736.84932, MAE_P: 0.3644486788817714, MAE_H: 154.11040641940403, Total Loss: 970711553.599706\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = create_instances_from_dataframe(dff)\n",
    "model = HalfLifeRegression()\n",
    "model.train(trainset)\n",
    "model.evaluate(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c352d4-5135-4cd7-8d65-117558086d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

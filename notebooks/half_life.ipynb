{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ee633-14e5-4fca-81b2-bda39cce72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half-Life as implemented by Duolingo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40860931-8de0-4f58-99e0-98d42682107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed1b7c-e173-48bc-9bf9-552d7b3ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "\n",
    "df_1 = df.merge(df_words, on = 'lexeme_id', how='inner')\n",
    "df_2 = df_1.merge(df_users, on = ['user_id', 'lang_combination'], how='inner')\n",
    "dff = df_2.drop(columns=['timestamp', 'lexeme_id', 'word', 'user_id', 'POS', 'person', 'number', 'gender', 'tense', 'def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c7b25-134a-49c2-8b8b-d775b192ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MIN_HALF_LIFE = 15.0 / (24 * 60)  # 15 minutes in days\n",
    "MAX_HALF_LIFE = 274.0            # 9 months\n",
    "LN2 = math.log(2)\n",
    "\n",
    "# Utility functions\n",
    "def pclip(p):\n",
    "    \"\"\"Clip recall probability to avoid numerical issues.\"\"\"\n",
    "    return min(max(p, 0.0001), 0.9999)\n",
    "\n",
    "def hclip(h):\n",
    "    \"\"\"Clip half-life to a reasonable range.\"\"\"\n",
    "    return min(max(h, MIN_HALF_LIFE), MAX_HALF_LIFE)\n",
    "\n",
    "def compute_half_life(p_recall, delta_t):\n",
    "    \"\"\"Compute target half-life from recall probability and time difference.\"\"\"\n",
    "    return hclip(-delta_t / math.log(p_recall, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893839da-8b8f-4e19-8b8c-4bbbdeb4499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HLR Model\n",
    "class HalfLifeRegression:\n",
    "    def __init__(self, learning_rate=0.001, hlwt=0.01, l2wt=0.1, sigma=1.):\n",
    "        self.weights = defaultdict(float)  # Feature weights\n",
    "        self.fcounts = defaultdict(int)    # Feature counts for adaptive learning rates\n",
    "        self.learning_rate = learning_rate # Base learning rate\n",
    "        self.hlwt = hlwt                   # Weight for half-life loss\n",
    "        self.l2wt = l2wt                   # L2 regularization weight\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def halflife(self, fv):\n",
    "        \"\"\"Compute predicted half-life based on feature vector.\"\"\"\n",
    "        try:\n",
    "            dp = sum([self.weights[k]*x_k for (k, x_k) in fv])\n",
    "            return hclip(2 ** dp)\n",
    "        except: \n",
    "            return MAX_HALF_LIFE\n",
    "            \n",
    "\n",
    "    def predict(self, fv, delta_t):\n",
    "        \"\"\"Predict recall probability and half-life.\"\"\"\n",
    "        h = self.halflife(fv)\n",
    "        p = 2 ** (-delta_t / h)\n",
    "        return pclip(p), h\n",
    "\n",
    "                \n",
    "     def train_update(self, p_true, delta_t, fv):\n",
    "        \"\"\"Update weights using one training instance.\"\"\"\n",
    "        h_true = compute_half_life(p_true, delta_t)\n",
    "        p_pred, h_pred = self.predict(fv, delta_t)\n",
    "\n",
    "        # Compute gradients\n",
    "        dlp_dw = 2 * (p_pred - p_true) * (LN2 ** 2) * p_pred * (delta_t / h_pred)\n",
    "        dlh_dw = 2 * (h_pred - h_true) * LN2 * h_pred\n",
    "\n",
    "        # Update weights\n",
    "        for k, x_k in fv:\n",
    "            rate = (1./(1+p_true)) * self.lrate / math.sqrt(1 + self.fcounts[k])\n",
    "            self.weights[k] -= rate * dlp_dw * x_k  # Update for recall probability loss\n",
    "            self.weights[k] -= rate * self.hlwt * dlh_dw * x_k  # Update for half-life loss\n",
    "            self.weights[k] -= rate * self.l2wt * self.weights[k] / self.sigma**2  # L2 regularization\n",
    "            self.fcounts[k] += 1\n",
    "\n",
    "\n",
    "    def train(self, dataset):\n",
    "        \"\"\"Train the model using the dataset.\"\"\"\n",
    "        for instance in dataset:\n",
    "            self.train_update(instance['p'], instance['delta_t'], instance['fv'])\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        \"\"\"Evaluate the model on a test dataset.\"\"\"\n",
    "        total_slp, total_slh = 0, 0\n",
    "        for instance in dataset:\n",
    "            p_pred, h_pred = self.predict(instance['fv'], instance['delta_t'])\n",
    "            slp = (instance['p'] - p_pred) ** 2\n",
    "            slh = (compute_half_life(instance['p'], instance['delta_t']) - h_pred) ** 2\n",
    "            total_slp += slp\n",
    "            total_slh += slh\n",
    "        print(f\"SLP Loss: {total_slp}, SLH Loss: {total_slh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d53767-091e-4113-9ab8-3960574efb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def train_update(self, inst):\n",
    "    #     if self.method == 'hlr':\n",
    "    #         base = 2.\n",
    "    #         p, h = self.predict(inst, base)\n",
    "    #         dlp_dw = 2.*(p-inst.p)*(LN2**2)*p*(inst.t/h)\n",
    "    #         dlh_dw = 2.*(h-inst.h)*LN2*h\n",
    "    #         for (k, x_k) in inst.fv:\n",
    "    #             rate = (1./(1+inst.p)) * self.lrate / math.sqrt(1 + self.fcounts[k])\n",
    "    #             # rate = self.lrate / math.sqrt(1 + self.fcounts[k])\n",
    "    #             # sl(p) update\n",
    "    #             self.weights[k] -= rate * dlp_dw * x_k\n",
    "    #             # sl(h) update\n",
    "    #             if not self.omit_h_term:\n",
    "    #                 self.weights[k] -= rate * self.hlwt * dlh_dw * x_k\n",
    "    #             # L2 regularization update\n",
    "    #             self.weights[k] -= rate * self.l2wt * self.weights[k] / self.sigma**2\n",
    "    #             # increment feature count for learning rate\n",
    "    #             self.fcounts[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42723394-33b0-4a81-9615-d8a8941a7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HalfLifeRegression()\n",
    "model.train(dataset)\n",
    "model.evaluate(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

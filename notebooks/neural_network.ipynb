{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "2ae4cfaa-e895-4cfb-8750-5922fd9f0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "#keras.layers.Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "97afe127-bec6-41b2-b52d-f32639c50d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def pclip(p):\n",
    "    \"\"\"Clip recall probability to avoid numerical issues.\"\"\"\n",
    "    return p.clip(0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def hclip(h):\n",
    "    min_half_life = 15.0 / (24 * 60)  # 15 minutes in days\n",
    "    max_half_life = 274.0   \n",
    "    \"\"\"Clip half-life to a reasonable range.\"\"\"\n",
    "    return h.clip(min_half_life, max_half_life)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86d645-8db6-4907-a100-5e4254c38dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "dff = pd.merge(pd.merge(df_words, df, on = 'lexeme_id', how='inner'), df_users, on = ['user_id', 'lang_combination'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d858cdc-e930-44a2-ac43-92362b9aa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['lexeme_id' ,'gender', 'def', 'tense', 'POS', 'person', 'number', 'word', 'session_seen', 'session_correct', 'avg_user_p_recall', 'timestamp', 'user_id', 'learning_language', 'ui_language']\n",
    "dff.drop(columns=cols_to_drop, inplace=True)\n",
    "dff.dropna(inplace=True)\n",
    "\n",
    "dff['p_recall'] = pclip(dff['p_recall'])\n",
    "# dff['half_life'] = hclip(-dff['delta']/np.log2(dff['p_recall']))\n",
    "dff['delta'] = dff['delta']/(60*60*24) # convert time delta to days\n",
    "dff['avg_delta'] = dff['avg_delta']/(60*60*24) \n",
    "dff['std_delta'] = dff['std_delta']/(60*60*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b2c84-b914-493f-b89e-f6b6b5ac9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c933ebf-5ef6-443b-af86-0d4eb4387a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_encoder = LabelEncoder()\n",
    "lang_encoder = LabelEncoder()\n",
    "\n",
    "dff['tags_list'] = tag_encoder.fit_transform(dff['tags_list'])\n",
    "dff['lang_combination'] = lang_encoder.fit_transform(dff['lang_combination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c5992-95b8-4d23-9394-3302a4ade04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    categorical_features = df.select_dtypes(include='O').columns\n",
    "    numeric_features = df.select_dtypes(exclude=['O']).columns.drop(['p_recall'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "    return df, categorical_features, numeric_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7885884-1b32-4ae6-bf1a-4155e6f19e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_1 = dff.copy()\n",
    "dff_1, categorical_features, numeric_features = prepare_dataset(dff_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778e4bb-6557-4c24-9cba-a01bb815fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "def split(df, numeric_features): \n",
    "    X = df.drop(columns=['p_recall'])\n",
    "    y = df['p_recall']\n",
    "    X_train, X_test, y_train, y_test = sklearn_train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=42)\n",
    "    X_train_tags = X_train['tags_list']\n",
    "    X_train_langs = X_train['lang_combination']\n",
    "    X_train_numerical = X_train[numeric_features] \n",
    "    \n",
    "    X_test_tags = X_test['tags_list']\n",
    "    X_test_langs = X_test['lang_combination']\n",
    "    X_test_numerical = X_test[numeric_features] \n",
    "\n",
    "    # In case we use half-life regression\n",
    "    # y_train_p_recall = y_train['p_recall']\n",
    "    # y_train_half_life = y_train['half_life']\n",
    "    # y_test_p_recall = y_test['p_recall']\n",
    "    # y_test_half_life = y_test['half_life']\n",
    "\n",
    "\n",
    "    \n",
    "    return df, X_train_tags, X_train_langs, X_train_numerical, X_test_tags, X_test_langs, X_test_numerical, X_test, y_train, y_test\n",
    "    # y_train_half_life, y_train_p_recall, y_test_half_life, y_test_p_recall\n",
    "\n",
    "df_final, X_train_tags, X_train_langs, X_train_numerical,X_test_tags, X_test_langs, X_test_numerical, X_test, y_train, y_test = split(dff_1.sample(frac=0.1), numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6ddc4-7782-4a1d-b08b-e17e4d162ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('X_train_tags_size', X_train_tags.shape)\n",
    "# print('X_train_langs_size', X_train_langs.shape)\n",
    "# print('X_train_numerical_size', X_train_numerical.shape)\n",
    "# print('X_test_tags_size', X_test_tags.shape)\n",
    "# print('X_test_langs_size', X_test_langs.shape)\n",
    "# print('X_test_numerical_size', X_test_numerical.shape)\n",
    "# print('y_train_half_life_size', y_train_half_life.shape)\n",
    "# print('y_train_p_recall_size', y_train_p_recall.shape)\n",
    "# print('y_test_half_life_size', y_test_half_life.size)\n",
    "# print('y_test_p_recall_size', y_test_p_recall.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e566998-4b71-4fc5-bb52-9a893bc3d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "len_tags = len(np.unique(df_final['tags_list']))\n",
    "len_langs = len(np.unique(df_final['lang_combination']))\n",
    "\n",
    "embedding_tags_size = int(min(np.ceil((len_tags)/2), 50))\n",
    "embedding_lang_size = int(min(np.ceil((len_langs)/2), 50))\n",
    "\n",
    "\n",
    "tags_input = Input(shape=(1,))  # Reshape input to (None, 1)\n",
    "langs_input = Input(shape=(1,))  # Reshape input to (None, 1)\n",
    "numerical_input = Input(shape=(len(numeric_features),))  # Should be 11\n",
    "\n",
    "tags_embedded = Embedding(input_dim=len_tags, output_dim=embedding_tags_size)(tags_input)  \n",
    "langs_embedded = Embedding(input_dim=len_langs, output_dim=embedding_langs_size)(langs_input)  \n",
    "\n",
    "flattened_tags = Flatten()(tags_embedded)\n",
    "flattened_langs = Flatten()(langs_embedded)\n",
    "\n",
    "# Concatenate layers\n",
    "conc = Concatenate()([flattened_tags, flattened_langs, numerical_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd9950-709b-422e-84b6-054b57d869f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Max tag index in X_train:\", X_train_tags.max(), \"Embedding input_dim:\", len_tags)\n",
    "# print(\"Max lang index in X_train:\", X_train_langs.max(), \"Embedding input_dim:\", len_langs)\n",
    "# print(\"Unique values in X_train_tags:\", np.unique(X_train_tags))\n",
    "# print(\"Unique values in X_train_langs:\", np.unique(X_train_langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cae0bc-b67e-4f69-8e11-92cf6cd5301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architectture\n",
    "# input_dim = X_train.shape[1] \n",
    "hidden_dim = 4         \n",
    "l2wt = 0.1              # L2 regularization weight\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "x = Dense(hidden_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(l2wt))(conc)\n",
    "output = Dense(1, activation=\"sigmoid\")(x) \n",
    "\n",
    "# p_recall_output = Dense(1, activation=\"sigmoid\", name=\"p_recall\")(x)\n",
    "# half_life_output = Dense(1, activation=\"relu\", name=\"half_life\")(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43187307-02b3-4ae2-9d3d-c7273e9a76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhlr_loss(y_true, y_pred):\n",
    "    p_true, h_true = y_true[:, 0], y_true[:, 1]\n",
    "    p_pred, h_pred = y_pred[:, 0], y_pred[:, 1]\n",
    "\n",
    "    slp = tf.reduce_mean(tf.square(p_true - p_pred)) # p_recall loss \n",
    "    slh = tf.reduce_mean(tf.square(h_true - h_pred)) # half-life loss \n",
    "\n",
    "    return slp + slh \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b1641-b989-43c9-b236-07bc97d9c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tags Input Shape:\", tags_input.shape)\n",
    "# print(\"Langs Input Shape:\", langs_input.shape)\n",
    "# print(\"Numerical Input Shape:\", numerical_input.shape)\n",
    "\n",
    "# print(\"Flattened tags Embedded Shape:\", flattened_tags.shape)\n",
    "# print(\"Flattened Langs Embedded Shape:\", flattened_langs.shape)\n",
    "# print(\"Flattened Numerical Input Shape:\", numerical_input.shape)\n",
    "\n",
    "# print(\"X_train_tags shape:\", X_train_tags.shape)    # Should be (batch_size, 1)\n",
    "# print(\"X_train_langs shape:\", X_train_langs.shape)  # Should be (batch_size, 1)\n",
    "# print(\"X_train_numerical shape:\", X_train_numerical.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0bccc-e1c5-454d-8978-8ea1d6e27604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[tags_input, langs_input, numerical_input], outputs=output)\n",
    "model.compile(loss=MeanSquaredError(), optimizer= Adam(learning_rate=learning_rate), metrics=['MAE'])\n",
    "model.fit([X_train_tags, X_train_langs, X_train_numerical], y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781b244-4446-4aa1-a79f-d62d48df995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_test_tags, X_test_langs, X_test_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db3717-0bec-4470-aa3f-ed8d75e8dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['p_recall_pred'] = y_pred_p_recall\n",
    "y_test['half_life_pred'] = y_pred_half_life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214bfcc-093b-4913-9ff8-94ec723e3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6adb3c-315b-4359-8262-84d5b80c7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
    "# y_test['p_recall_pred'] = y_pred\n",
    "# dff['half_life_pred'] = y_pred\n",
    "\n",
    "mae_p = np.mean(np.abs(results['Actual'] - results['Predicted']))\n",
    "# mae_h = np.mean(np.abs(dff['half_life'] - dff['half_life_pred']))\n",
    "\n",
    "print(f\"Final MAE - p_recall: {mae_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1b055626-5f5e-441d-b333-f0be9b6f70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tag index: 400 Embedding input_dim: 361\n",
      "Max lang index: 7 Embedding input_dim: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Max tag index:\", X_train_tags.max(), \"Embedding input_dim:\", len_tags)\n",
    "print(\"Max lang index:\", X_train_langs.max(), \"Embedding input_dim:\", len_langs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398f546-bf99-4641-84c7-e7be5ada8bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

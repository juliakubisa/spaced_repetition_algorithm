{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ae4cfaa-e895-4cfb-8750-5922fd9f0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13abe128-1b7b-4f8f-8edf-488acc907f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b86d645-8db6-4907-a100-5e4254c38dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "filename = 'df_processed.csv'\n",
    "filepath = os.path.normpath(os.path.join(current_dir, '../data/processed/', filename))\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "df_users = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'users_behaviur.csv')))\n",
    "df_words = pd.read_csv(os.path.normpath(os.path.join(current_dir, '../data/features/', 'word_complexity_features.csv')), sep='\\t')\n",
    "df_1 = df.merge(df_words, on = 'lexeme_id', how='inner')\n",
    "df_2 = df_1.merge(df_users, on = ['user_id', 'lang_combination'], how='inner')\n",
    "\n",
    "dff = df_2.drop(columns=['word', 'user_id', 'session_seen', 'session_correct', 'avg_user_p_recall', 'timestamp'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2b03ae3-61fd-4f99-acef-c1dae99f917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p_recall', 'delta', 'learning_language', 'ui_language', 'lexeme_id',\n",
       "       'history_seen', 'history_correct', 'h_recall', 'lang_combination',\n",
       "       'gender', 'def', 'tense', 'POS', 'person', 'number', 'word_len',\n",
       "       'tags_list', 'SUBTLEX', 'avg_delta', 'std_delta', 'avg_h_recall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aae8e1bb-b536-4c96-9618-73ba8c4744b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "dff[['p_recall', 'delta', 'history_seen', 'history_correct', 'h_recall',\n",
    "       'word_len', 'SUBTLEX', 'avg_delta', 'std_delta',\n",
    "       'avg_h_recall']] = scaler.fit_transform(dff[['p_recall', 'delta', 'history_seen', 'history_correct', 'h_recall',\n",
    "       'word_len', 'SUBTLEX', 'avg_delta', 'std_delta', 'avg_h_recall']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e2e987d-55b4-416d-9176-844ab3736c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(df):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical variables\n",
    "    \"\"\" \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    ohe_data = ohe.fit_transform(df[categorical_cols])\n",
    "    ohe_df = pd.DataFrame(ohe_data, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "    df_encoded = pd.concat([df.select_dtypes(exclude='O'), ohe_df], axis=1)\n",
    "    df_encoded.dropna(inplace=True)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42eb3a0-f08b-43d7-ad5d-bb9d0e069aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_encoded = ohe(dff.sample(1000000))\n",
    "X = dff_encoded.drop(columns='p_recall')\n",
    "y = dff_encoded['p_recall']\n",
    "X_train, X_test, y_train, y_test = sklearn_train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cae0bc-b67e-4f69-8e11-92cf6cd5301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1] \n",
    "hidden_dim = 4         \n",
    "# hlwt = 0.01             # Half-life loss weight\n",
    "l2wt = 0.1              # L2 regularization weight\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(hidden_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(l2wt), input_shape=(input_dim,)),\n",
    "    Dense(2)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43187307-02b3-4ae2-9d3d-c7273e9a76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhlr_loss(y_true, y_pred):\n",
    "    p_true, h_true = y_true[:, 0], y_true[:, 1]\n",
    "    p_pred, h_pred = y_pred[:, 0], y_pred[:, 1]\n",
    "\n",
    "    slp = tf.reduce_mean(tf.square(p_true - p_pred)) # p_recall loss \n",
    "    slh = tf.reduce_mean(tf.square(h_true - h_pred)) # half-life loss \n",
    "\n",
    "    return slp + slh \n",
    "    \n",
    "\n",
    "model.compile(loss=nhlr_loss, optimizer= Adam(learning_rate=learning_rate))\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0bccc-e1c5-454d-8978-8ea1d6e27604",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "df['p_recall_pred'] = y_pred[:, 0]\n",
    "df['half_life_pred'] = y_pred[:, 1]\n",
    "\n",
    "mae_p = np.mean(np.abs(df['p_recall'] - df['p_recall_pred']))\n",
    "mae_h = np.mean(np.abs(df['half_life'] - df['half_life_pred']))\n",
    "\n",
    "print(f\"Final MAE - p_recall: {mae_p:.4f}, half-life: {mae_h:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781b244-4446-4aa1-a79f-d62d48df995e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
